{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da623df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'text_processing' from 'd:\\\\classical-music-generation-model\\\\text_processing.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib\n",
    "import data_preprocessing, midi_conversion, model_helpers, models, text_processing\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config\n",
    ")\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(midi_conversion)\n",
    "importlib.reload(model_helpers)\n",
    "importlib.reload(models)\n",
    "importlib.reload(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafd936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = \"data/midi_text_exports\"\n",
    "VOCAB_FILE = \"data/midi_text_exports/midi_vocab.txt\"\n",
    "BLOCK_SIZE = 512\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_DIR = \"models/midi_gpt2_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6931e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading MIDIs from data\\train.\n",
      "Could not load data\\train\\beethoven-anhang_14_3.mid: Could not decode key with 3 flats and mode 255\n",
      "Could not load data\\train\\mozart-piano_sonatas-nueva_carpeta-k281_piano_sonata_n03_3mov.mid: Could not decode key with 2 flats and mode 2\n",
      "Could not load data\\train\\unknown_artist-i_o-mozart_k550.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 500 MIDI files from data\\train\n",
      "Now loading MIDIs from data\\val.\n",
      "Loaded 47 MIDI files from data\\val\n",
      "Now loading MIDIs from data\\test.\n",
      "Could not load data\\test\\unknown_artist-i_o-mozart_q1_2.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 43 MIDI files from data\\test\n",
      "590 MIDI files retrieved.\n",
      "Successfully processed 500 MIDIs into text.\n",
      "Successfully processed 47 MIDIs into text.\n",
      "Successfully processed 43 MIDIs into text.\n",
      "Saved 500 files to                       data/midi_text_exports\\train\n",
      "Saved 47 files to                       data/midi_text_exports\\val\n",
      "Saved 43 files to                       data/midi_text_exports\\test\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import get_midis_by_composer, midi_split_to_text_split\n",
    "\n",
    "composers = [\"mozart\", \"haydn\", \"beethoven\"]\n",
    "midis = get_midis_by_composer(composers)\n",
    "\n",
    "# [[train texts], [val texts], [test texts]]\n",
    "# Export dir: \"data/midi_text_exports\"\n",
    "midi_texts = midi_split_to_text_split(midis, save_to_directory=\"data/midi_text_exports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34d8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing vocab file: data/midi_text_exports/midi_vocab.txt\n",
      "MIDI vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "from text_processing import build_vocab_from_dir\n",
    "from text_processing import MidiTokenizer\n",
    "\n",
    "\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    print(f\"{VOCAB_FILE} not found, building from {DATA_DIR}...\")\n",
    "    counter = build_vocab_from_dir(DATA_DIR)\n",
    "    base_tokens = sorted(counter.keys())\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    vocab = specials + base_tokens\n",
    "    with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for tok in vocab:\n",
    "            f.write(tok + \"\\n\")\n",
    "    print(f\"Saved vocab with {len(vocab)} tokens to {VOCAB_FILE}\")\n",
    "else:\n",
    "    print(f\"Found existing vocab file: {VOCAB_FILE}\")\n",
    "\n",
    "tokenizer = MidiTokenizer(VOCAB_FILE)\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(\"MIDI vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e9acefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21696 sequences from data/midi_text_exports\\train\n",
      "Loaded 1962 sequences from data/midi_text_exports\\val\n"
     ]
    }
   ],
   "source": [
    "from text_processing import MidiTextDataset\n",
    "from model_helpers import collate_fn\n",
    "\n",
    "\n",
    "train_dataset = MidiTextDataset(os.path.join(DATA_DIR, \"train\"), tokenizer, block_size=BLOCK_SIZE)\n",
    "val_dataset   = MidiTextDataset(os.path.join(DATA_DIR, \"val\"),   tokenizer, block_size=BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2...\n",
      "GPT-2 hidden size: 768\n",
      "Model ready. New vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained GPT-2...\")\n",
    "base_model_name = \"gpt2\"  # you can try \"gpt2-medium\" if you have VRAM\n",
    "\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "base_config = pretrained_model.config\n",
    "\n",
    "hidden_size = base_config.n_embd\n",
    "print(\"GPT-2 hidden size:\", hidden_size)\n",
    "\n",
    "# New config: same architecture, new vocab size + pad/bos/eos\n",
    "new_config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=base_config.n_positions,\n",
    "    n_ctx=base_config.n_ctx,\n",
    "    n_embd=base_config.n_embd,\n",
    "    n_layer=base_config.n_layer,\n",
    "    n_head=base_config.n_head,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    resid_pdrop=0.2,  # for mitigating overfitting\n",
    "    embd_pdrop=0.2,\n",
    "    attn_pdrop=0.2,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(new_config)\n",
    "\n",
    "# Copy transformer blocks and positional embeddings from pretrained\n",
    "with torch.no_grad():\n",
    "    # positional embeddings\n",
    "    model.transformer.wpe.weight.copy_(pretrained_model.transformer.wpe.weight)\n",
    "\n",
    "    # transformer blocks\n",
    "    for new_block, old_block in zip(model.transformer.h, pretrained_model.transformer.h):\n",
    "        new_block.load_state_dict(old_block.state_dict())\n",
    "\n",
    "    # final layer norm\n",
    "    model.transformer.ln_f.load_state_dict(pretrained_model.transformer.ln_f.state_dict())\n",
    "\n",
    "    # We intentionally leave token embeddings (wte) and lm_head randomly initialized\n",
    "    # to match new vocab.\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model ready. New vocab size:\", model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a06d67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 904/904 [09:51<00:00,  1.53it/s, batch_loss=1.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | train loss:               2.6200 | val loss: 1.1237\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 904/904 [09:43<00:00,  1.55it/s, batch_loss=0.9353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | train loss:               0.9596 | val loss: 0.9537\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.7254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | train loss:               0.8253 | val loss: 0.8794\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.8048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | train loss:               0.7546 | val loss: 0.8527\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 904/904 [09:43<00:00,  1.55it/s, batch_loss=0.8338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | train loss:               0.7061 | val loss: 0.8355\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.6464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | train loss:               0.6669 | val loss: 0.8307\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.6638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | train loss:               0.6322 | val loss: 0.8278\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 904/904 [09:45<00:00,  1.54it/s, batch_loss=0.6333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | train loss:               0.5998 | val loss: 0.8327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.6043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | train loss:               0.5697 | val loss: 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.6018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | train loss:               0.5413 | val loss: 0.8548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.4291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | train loss:               0.5149 | val loss: 0.8657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 904/904 [09:45<00:00,  1.54it/s, batch_loss=0.4671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | train loss:               0.4902 | val loss: 0.8791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 904/904 [09:43<00:00,  1.55it/s, batch_loss=0.4583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | train loss:               0.4675 | val loss: 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 904/904 [09:43<00:00,  1.55it/s, batch_loss=0.4454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | train loss:               0.4466 | val loss: 0.9109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 904/904 [10:00<00:00,  1.51it/s, batch_loss=0.6344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | train loss:               0.4278 | val loss: 0.9225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 904/904 [09:59<00:00,  1.51it/s, batch_loss=0.4723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | train loss:               0.4108 | val loss: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 904/904 [09:45<00:00,  1.54it/s, batch_loss=0.5193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | train loss:               0.3961 | val loss: 0.9587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 904/904 [09:45<00:00,  1.54it/s, batch_loss=0.4008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | train loss:               0.3831 | val loss: 0.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 904/904 [09:44<00:00,  1.55it/s, batch_loss=0.3547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | train loss:               0.3729 | val loss: 0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 904/904 [09:50<00:00,  1.53it/s, batch_loss=0.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | train loss:               0.3648 | val loss: 0.9856\n"
     ]
    }
   ],
   "source": [
    "from models import train_gpt_2\n",
    "\n",
    "train_gpt_2(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY, device=DEVICE, model_save_dir=MODEL_SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "945e2ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Example generation\u001b[39;00m\n\u001b[32m      6\u001b[39m example_prompt = \u001b[33m\"\u001b[39m\u001b[33m<SOS> COMPOSER_beethoven\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m generated_tokens = \u001b[43mgenerate_midi_tokens_with_gpt_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexample_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVOCAB_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_SAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1021\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated token sequence:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(generated_tokens[:\u001b[32m100\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generated_tokens) > \u001b[32m500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\models.py:551\u001b[39m, in \u001b[36mgenerate_midi_tokens_with_gpt_model\u001b[39m\u001b[34m(prompt_text, vocab_file, model_save_dir, max_new_tokens, temp, top_k, device)\u001b[39m\n\u001b[32m    548\u001b[39m     device = util.get_best_device()\n\u001b[32m    550\u001b[39m tok = MidiTokenizer(vocab_file)\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m mdl = \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m mdl.eval()\n\u001b[32m    554\u001b[39m prompt_ids = tok.encode(prompt_text, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from models import generate_midi_tokens_with_gpt_model\n",
    "from midi_conversion import text_to_midi\n",
    "import util\n",
    "\n",
    "# Example generation\n",
    "example_prompt = \"<SOS> COMPOSER_beethoven\"\n",
    "generated_tokens = generate_midi_tokens_with_gpt_model(\n",
    "    example_prompt, VOCAB_FILE, MODEL_SAVE_DIR, max_new_tokens=1021, temp=1.0)\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_tokens[:100], \"...\" if len(generated_tokens) > 500 else \"\")\n",
    "\n",
    "# Convert generated text to MIDI and save\n",
    "generated_mid = text_to_midi(generated_tokens)\n",
    "util.mkdir(\"generated\")\n",
    "midi_path = util.path_join(\"generated\", \"gpt2_generated_sample.mid\")\n",
    "generated_mid.save(midi_path)\n",
    "print(\"Saved generated MIDI to:\", midi_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
