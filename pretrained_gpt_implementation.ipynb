{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da623df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'text_processing' from 'd:\\\\classical-music-generation-model\\\\text_processing.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib\n",
    "import data_preprocessing, midi_conversion, model_helpers, models, text_processing\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config\n",
    ")\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(midi_conversion)\n",
    "importlib.reload(model_helpers)\n",
    "importlib.reload(models)\n",
    "importlib.reload(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daafd936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = \"data/midi_text_exports\"\n",
    "VOCAB_FILE = \"data/midi_text_exports/midi_vocab.txt\"\n",
    "BLOCK_SIZE = 512\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_DIR = \"models/midi_gpt2_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6931e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading MIDIs from data\\train.\n",
      "Could not load data\\train\\beethoven-anhang_14_3.mid: Could not decode key with 3 flats and mode 255\n",
      "Could not load data\\train\\mozart-piano_sonatas-nueva_carpeta-k281_piano_sonata_n03_3mov.mid: Could not decode key with 2 flats and mode 2\n",
      "Could not load data\\train\\unknown_artist-i_o-mozart_k550.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 500 MIDI files from data\\train\n",
      "Now loading MIDIs from data\\val.\n",
      "Loaded 47 MIDI files from data\\val\n",
      "Now loading MIDIs from data\\test.\n",
      "Could not load data\\test\\unknown_artist-i_o-mozart_q1_2.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 43 MIDI files from data\\test\n",
      "590 MIDI files retrieved.\n",
      "Successfully processed 500 MIDIs into text.\n",
      "Successfully processed 47 MIDIs into text.\n",
      "Successfully processed 43 MIDIs into text.\n",
      "Saved 500 files to                       data/midi_text_exports\\train\n",
      "Saved 47 files to                       data/midi_text_exports\\val\n",
      "Saved 43 files to                       data/midi_text_exports\\test\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import get_midis_by_composer, midi_split_to_text_split\n",
    "\n",
    "composers = [\"mozart\", \"haydn\", \"beethoven\"]\n",
    "midis = get_midis_by_composer(composers)\n",
    "\n",
    "# [[train texts], [val texts], [test texts]]\n",
    "# Export dir: \"data/midi_text_exports\"\n",
    "midi_texts = midi_split_to_text_split(midis, save_to_directory=\"data/midi_text_exports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34d8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing vocab file: data/midi_text_exports/midi_vocab.txt\n",
      "MIDI vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "from text_processing import build_vocab_from_dir\n",
    "from text_processing import MidiTokenizer\n",
    "\n",
    "\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    print(f\"{VOCAB_FILE} not found, building from {DATA_DIR}...\")\n",
    "    counter = build_vocab_from_dir(DATA_DIR)\n",
    "    base_tokens = sorted(counter.keys())\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    vocab = specials + base_tokens\n",
    "    with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for tok in vocab:\n",
    "            f.write(tok + \"\\n\")\n",
    "    print(f\"Saved vocab with {len(vocab)} tokens to {VOCAB_FILE}\")\n",
    "else:\n",
    "    print(f\"Found existing vocab file: {VOCAB_FILE}\")\n",
    "\n",
    "tokenizer = MidiTokenizer(VOCAB_FILE)\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(\"MIDI vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9acefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21696 sequences from data/midi_text_exports\\train\n",
      "Loaded 1962 sequences from data/midi_text_exports\\val\n"
     ]
    }
   ],
   "source": [
    "from text_processing import MidiTextDataset\n",
    "from model_helpers import collate_fn\n",
    "\n",
    "\n",
    "train_dataset = MidiTextDataset(os.path.join(DATA_DIR, \"train\"), tokenizer, block_size=BLOCK_SIZE)\n",
    "val_dataset   = MidiTextDataset(os.path.join(DATA_DIR, \"val\"),   tokenizer, block_size=BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416a514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2...\n",
      "GPT-2 hidden size: 768\n",
      "Model ready. New vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained GPT-2...\")\n",
    "base_model_name = \"gpt2\"\n",
    "\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "base_config = pretrained_model.config\n",
    "\n",
    "hidden_size = base_config.n_embd\n",
    "print(\"GPT-2 hidden size:\", hidden_size)\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=base_config.n_positions,\n",
    "    n_ctx=base_config.n_ctx,\n",
    "    n_embd=base_config.n_embd,\n",
    "    n_layer=base_config.n_layer,\n",
    "    n_head=base_config.n_head,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    resid_pdrop=0.2,  # for mitigating overfitting\n",
    "    embd_pdrop=0.2,\n",
    "    attn_pdrop=0.2,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Copy transformer blocks and positional embeddings from pretrained\n",
    "with torch.no_grad():\n",
    "    # positional embeddings\n",
    "    model.transformer.wpe.weight.copy_(pretrained_model.transformer.wpe.weight)\n",
    "\n",
    "    # transformer blocks\n",
    "    for new_block, old_block in zip(model.transformer.h, pretrained_model.transformer.h):\n",
    "        new_block.load_state_dict(old_block.state_dict())\n",
    "\n",
    "    # final layer norm\n",
    "    model.transformer.ln_f.load_state_dict(pretrained_model.transformer.ln_f.state_dict())\n",
    "\n",
    "    # We intentionally leave token embeddings (wte) and lm_head randomly initialized\n",
    "    # to match new vocab.\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model ready. New vocab size:\", model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06d67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/904 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "Epoch 1/20: 100%|██████████| 904/904 [09:55<00:00,  1.52it/s, batch_loss=1.3197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | train loss:               2.9362 | val loss: 1.1838\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 904/904 [09:57<00:00,  1.51it/s, batch_loss=0.9003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | train loss:               1.0451 | val loss: 0.9974\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 904/904 [09:56<00:00,  1.51it/s, batch_loss=0.9492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | train loss:               0.8934 | val loss: 0.9128\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 904/904 [09:49<00:00,  1.53it/s, batch_loss=0.7193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | train loss:               0.8196 | val loss: 0.8757\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 904/904 [09:53<00:00,  1.52it/s, batch_loss=0.7589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | train loss:               0.7721 | val loss: 0.8528\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 904/904 [09:46<00:00,  1.54it/s, batch_loss=0.6977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | train loss:               0.7367 | val loss: 0.8412\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 904/904 [09:54<00:00,  1.52it/s, batch_loss=0.7371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | train loss:               0.7077 | val loss: 0.8269\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 904/904 [09:46<00:00,  1.54it/s, batch_loss=0.7448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | train loss:               0.6821 | val loss: 0.8296\n",
      "No improvement in val loss for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 904/904 [09:47<00:00,  1.54it/s, batch_loss=0.6226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | train loss:               0.6592 | val loss: 0.8240\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 904/904 [09:52<00:00,  1.52it/s, batch_loss=0.6080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | train loss:               0.6385 | val loss: 0.8264\n",
      "No improvement in val loss for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 904/904 [09:50<00:00,  1.53it/s, batch_loss=0.4933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | train loss:               0.6199 | val loss: 0.8190\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 904/904 [09:51<00:00,  1.53it/s, batch_loss=0.5760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | train loss:               0.6023 | val loss: 0.8194\n",
      "No improvement in val loss for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 904/904 [09:49<00:00,  1.53it/s, batch_loss=0.6017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | train loss:               0.5869 | val loss: 0.8296\n",
      "No improvement in val loss for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 904/904 [09:53<00:00,  1.52it/s, batch_loss=0.5280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | train loss:               0.5724 | val loss: 0.8302\n",
      "No improvement in val loss for 3 epoch(s).\n",
      "Early stopping triggered: no val-loss improvement for 3                         epochs.\n",
      "Training complete. Best val loss: 0.8190 at epoch 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_losses': [2.9362449400720343,\n",
       "  1.0451273052433951,\n",
       "  0.8933934326720449,\n",
       "  0.8196259317672359,\n",
       "  0.7721305268000712,\n",
       "  0.7367371545560593,\n",
       "  0.7077136263251305,\n",
       "  0.6820601439515573,\n",
       "  0.6592394471234453,\n",
       "  0.6385357220352224,\n",
       "  0.6198714019780138,\n",
       "  0.6023377043285728,\n",
       "  0.5868506243996388,\n",
       "  0.5723666941038276],\n",
       " 'val_losses': [1.1838221390072892,\n",
       "  0.9973697291641701,\n",
       "  0.9127541882235829,\n",
       "  0.8757176675447603,\n",
       "  0.8527709754501901,\n",
       "  0.8411976560586836,\n",
       "  0.8269084248600936,\n",
       "  0.8296223806171883,\n",
       "  0.8240174240455395,\n",
       "  0.8264162758501564,\n",
       "  0.8190460681188397,\n",
       "  0.8194475988062416,\n",
       "  0.829629559705897,\n",
       "  0.8302457176330613],\n",
       " 'best_val_loss': 0.8190460681188397,\n",
       " 'best_epoch': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import train_gpt_2\n",
    "\n",
    "train_gpt_2(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY, device=DEVICE, model_save_dir=MODEL_SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945e2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token sequence:\n",
      "<SOS> COMPOSER_beethoven KEY_F TIME_SIGNATURE_6/8 TEMPO_BPM_36 MEASURE BEAT POS_0 NOTE_60 DUR_24 VEL ...\n",
      "Saved generated MIDI to: generated\\gpt2_generated_sample.mid\n"
     ]
    }
   ],
   "source": [
    "from models import generate_midi_tokens_with_gpt_model\n",
    "from midi_conversion import text_to_midi\n",
    "import util\n",
    "\n",
    "# Example generation\n",
    "prompt = \"\"  # must decrease max_new_tokens if adding prompt\n",
    "generated_tokens = generate_midi_tokens_with_gpt_model(\n",
    "    prompt, VOCAB_FILE, MODEL_SAVE_DIR, max_new_tokens=1024, temp=1.0)\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_tokens[:100], \"...\" if len(generated_tokens) > 500 else \"\")\n",
    "\n",
    "# Convert generated text to MIDI and save\n",
    "generated_mid = text_to_midi(generated_tokens)\n",
    "util.mkdir(\"generated\")\n",
    "midi_path = util.path_join(\"generated\", \"gpt2_generated_sample.mid\")\n",
    "generated_mid.save(midi_path)\n",
    "print(\"Saved generated MIDI to:\", midi_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
