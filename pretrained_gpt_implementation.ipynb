{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da623df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'text_processing' from 'd:\\\\classical-music-generation-model\\\\text_processing.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib\n",
    "import data_preprocessing, midi_conversion, model_helpers, models, text_processing\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(midi_conversion)\n",
    "importlib.reload(model_helpers)\n",
    "importlib.reload(models)\n",
    "importlib.reload(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daafd936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = \"data/midi_text_exports\"\n",
    "VOCAB_FILE = \"data/midi_text_exports/midi_vocab.txt\"\n",
    "BLOCK_SIZE = 512\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_DIR = \"midi_gpt2_model\"\n",
    "TOKENIZER_SAVE_DIR = \"midi_gpt2_tokenizer\"\n",
    "\n",
    "from midi_conversion import text_to_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6931e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading MIDIs from data\\train.\n",
      "Could not load data\\train\\beethoven-anhang_14_3.mid: Could not decode key with 3 flats and mode 255\n",
      "Could not load data\\train\\mozart-piano_sonatas-nueva_carpeta-k281_piano_sonata_n03_3mov.mid: Could not decode key with 2 flats and mode 2\n",
      "Could not load data\\train\\unknown_artist-i_o-mozart_k550.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 500 MIDI files from data\\train\n",
      "Now loading MIDIs from data\\val.\n",
      "Loaded 47 MIDI files from data\\val\n",
      "Now loading MIDIs from data\\test.\n",
      "Could not load data\\test\\unknown_artist-i_o-mozart_q1_2.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 43 MIDI files from data\\test\n",
      "590 MIDI files retrieved.\n",
      "Successfully processed 500 MIDIs into text.\n",
      "Successfully processed 47 MIDIs into text.\n",
      "Successfully processed 43 MIDIs into text.\n",
      "Saved 500 files to                       data/midi_text_exports\\train\n",
      "Saved 47 files to                       data/midi_text_exports\\val\n",
      "Saved 43 files to                       data/midi_text_exports\\test\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import get_midis_by_composer, midi_split_to_text_split\n",
    "\n",
    "composers = [\"mozart\", \"haydn\", \"beethoven\"]\n",
    "midis = get_midis_by_composer(composers)\n",
    "\n",
    "# [[train texts], [val texts], [test texts]]\n",
    "# Export dir: \"data/midi_text_exports\"\n",
    "midi_texts = midi_split_to_text_split(midis, save_to_directory=\"data/midi_text_exports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f34d8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing vocab file: data/midi_text_exports/midi_vocab.txt\n",
      "MIDI vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "from text_processing import build_vocab_from_dir\n",
    "from text_processing import MidiTokenizer\n",
    "\n",
    "\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    print(f\"{VOCAB_FILE} not found, building from {DATA_DIR}...\")\n",
    "    counter = build_vocab_from_dir(DATA_DIR)\n",
    "    base_tokens = sorted(counter.keys())\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    vocab = specials + base_tokens\n",
    "    with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for tok in vocab:\n",
    "            f.write(tok + \"\\n\")\n",
    "    print(f\"Saved vocab with {len(vocab)} tokens to {VOCAB_FILE}\")\n",
    "else:\n",
    "    print(f\"Found existing vocab file: {VOCAB_FILE}\")\n",
    "\n",
    "tokenizer = MidiTokenizer(VOCAB_FILE)\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(\"MIDI vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e9acefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21696 sequences from data/midi_text_exports\\train\n",
      "Loaded 1962 sequences from data/midi_text_exports\\val\n"
     ]
    }
   ],
   "source": [
    "from text_processing import MidiTextDataset\n",
    "from model_helpers import collate_fn\n",
    "\n",
    "\n",
    "train_dataset = MidiTextDataset(os.path.join(DATA_DIR, \"train\"), tokenizer, block_size=BLOCK_SIZE)\n",
    "val_dataset   = MidiTextDataset(os.path.join(DATA_DIR, \"val\"),   tokenizer, block_size=BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416a514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2...\n",
      "GPT-2 hidden size: 768\n",
      "Model ready. New vocab size: 701\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained GPT-2...\")\n",
    "base_model_name = \"gpt2\"  # you can try \"gpt2-medium\" if you have VRAM\n",
    "\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "base_config = pretrained_model.config\n",
    "\n",
    "hidden_size = base_config.n_embd\n",
    "print(\"GPT-2 hidden size:\", hidden_size)\n",
    "\n",
    "# New config: same architecture, new vocab size + pad/bos/eos\n",
    "new_config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=base_config.n_positions,\n",
    "    n_ctx=base_config.n_ctx,\n",
    "    n_embd=base_config.n_embd,\n",
    "    n_layer=base_config.n_layer,\n",
    "    n_head=base_config.n_head,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(new_config)\n",
    "\n",
    "# Copy transformer blocks and positional embeddings from pretrained\n",
    "with torch.no_grad():\n",
    "    # positional embeddings\n",
    "    model.transformer.wpe.weight.copy_(pretrained_model.transformer.wpe.weight)\n",
    "\n",
    "    # transformer blocks\n",
    "    for new_block, old_block in zip(model.transformer.h, pretrained_model.transformer.h):\n",
    "        new_block.load_state_dict(old_block.state_dict())\n",
    "\n",
    "    # final layer norm\n",
    "    model.transformer.ln_f.load_state_dict(pretrained_model.transformer.ln_f.state_dict())\n",
    "\n",
    "    # We intentionally leave token embeddings (wte) and lm_head randomly initialized\n",
    "    # to match new vocab.\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model ready. New vocab size:\", model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a06d67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 9/904 [00:06<10:54,  1.37it/s, batch_loss=7.0689]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_gpt_2\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain_gpt_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_SAVE_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\classical-music-generation-model\\models.py:487\u001b[39m, in \u001b[36mtrain_gpt_2\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, device, model_save_dir)\u001b[39m\n\u001b[32m    484\u001b[39m     optimizer.step()\n\u001b[32m    485\u001b[39m     scheduler.step()\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     total_train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mbatch_loss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m})\n\u001b[32m    491\u001b[39m avg_train_loss = total_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from models import train_gpt_2\n",
    "\n",
    "train_gpt_2(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY, device=DEVICE, model_save_dir=MODEL_SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945e2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token sequence:\n",
      "<SOS> COMPOSER_mozart KEY_C TIME_SIGNATURE_4/4 TEMPO_BPM_112 MEASURE BEAT POS_0 NOTE_36 DUR_22 VEL_6 ...\n",
      "Saved generated MIDI to: generated\\gpt2_generated_sample.mid\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import torch, os\n",
    "\n",
    "def generate_midi_tokens(\n",
    "    prompt_text: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "):\n",
    "    tok = MidiTokenizer(VOCAB_FILE)\n",
    "    mdl = GPT2LMHeadModel.from_pretrained(MODEL_SAVE_DIR).to(DEVICE)\n",
    "    mdl.eval()\n",
    "\n",
    "    prompt_ids = tok.encode(prompt_text, add_special_tokens=False)\n",
    "\n",
    "    # Build input: [BOS] + prompt tokens  (no EOS)\n",
    "    input_ids = torch.tensor([[tok.bos_token_id] + prompt_ids], dtype=torch.long).to(DEVICE)\n",
    "    attention_mask = (input_ids != tok.pad_token_id).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = mdl.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = output_ids[0].tolist()\n",
    "    generated_text = tok.decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example prompt\n",
    "example_prompt = \"\"\n",
    "generated_tokens = generate_midi_tokens(example_prompt, max_new_tokens=1024)\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_tokens[:100], \"...\" if len(generated_tokens) > 500 else \"\")\n",
    "\n",
    "# Convert generated text to MIDI and save\n",
    "generated_mid = text_to_midi(generated_tokens)\n",
    "os.makedirs(\"generated\", exist_ok=True)\n",
    "midi_path = os.path.join(\"generated\", \"gpt2_generated_sample.mid\")\n",
    "generated_mid.save(midi_path)\n",
    "print(\"Saved generated MIDI to:\", midi_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
