{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da623df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'text_processing' from 'd:\\\\classical-music-generation-model\\\\text_processing.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import importlib\n",
    "import data_preprocessing, midi_conversion, model_helpers, models, text_processing\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(midi_conversion)\n",
    "importlib.reload(model_helpers)\n",
    "importlib.reload(models)\n",
    "importlib.reload(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daafd936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = \"data/midi_text_exports\"\n",
    "VOCAB_FILE = \"data/midi_text_exports/midi_vocab.txt\"\n",
    "BLOCK_SIZE = 512\n",
    "BATCH_SIZE = 24\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MODEL_SAVE_DIR = \"midi_gpt2_model\"\n",
    "TOKENIZER_SAVE_DIR = \"midi_gpt2_tokenizer\"\n",
    "\n",
    "from midi_conversion import text_to_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6931e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading MIDIs from data\\train.\n",
      "Could not load data\\train\\beethoven-anhang_14_3.mid: Could not decode key with 3 flats and mode 255\n",
      "Could not load data\\train\\mozart-piano_sonatas-nueva_carpeta-k281_piano_sonata_n03_3mov.mid: Could not decode key with 2 flats and mode 2\n",
      "Could not load data\\train\\unknown_artist-i_o-mozart_k550.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 500 MIDI files from data\\train\n",
      "Now loading MIDIs from data\\val.\n",
      "Loaded 47 MIDI files from data\\val\n",
      "Now loading MIDIs from data\\test.\n",
      "Could not load data\\test\\unknown_artist-i_o-mozart_q1_2.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 43 MIDI files from data\\test\n",
      "590 MIDI files retrieved.\n",
      "Successfully processed 500 MIDIs into text.\n",
      "Successfully processed 47 MIDIs into text.\n",
      "Successfully processed 43 MIDIs into text.\n",
      "Saved 500 files to                       data/midi_text_exports\\train\n",
      "Saved 47 files to                       data/midi_text_exports\\val\n",
      "Saved 43 files to                       data/midi_text_exports\\test\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import get_midis_by_composer, midi_split_to_text_split\n",
    "\n",
    "composers = [\"mozart\", \"haydn\", \"beethoven\"]\n",
    "midis = get_midis_by_composer(composers)\n",
    "\n",
    "# [[train texts], [val texts], [test texts]]\n",
    "# Export dir: \"data/midi_text_exports\"\n",
    "midi_texts = midi_split_to_text_split(midis, save_to_directory=\"data/midi_text_exports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing vocab file: data/midi_text_exports/midi_vocab.txt\n",
      "data/midi_text_exports/midi_vocab.txt not found, building from data/midi_text_exports...\n",
      "Saved vocab with 640 tokens to data/midi_text_exports/midi_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from text_processing import build_vocab_from_dir\n",
    "\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    print(f\"{VOCAB_FILE} not found, building from {DATA_DIR}...\")\n",
    "    counter = build_vocab_from_dir(DATA_DIR)\n",
    "    base_tokens = sorted(counter.keys())\n",
    "    specials = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\n",
    "    vocab = specials + base_tokens\n",
    "    with open(VOCAB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        for tok in vocab:\n",
    "            f.write(tok + \"\\n\")\n",
    "    print(f\"Saved vocab with {len(vocab)} tokens to {VOCAB_FILE}\")\n",
    "else:\n",
    "    print(f\"Found existing vocab file: {VOCAB_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "932344b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIDI vocab size: 640\n"
     ]
    }
   ],
   "source": [
    "from text_processing import MidiTokenizer\n",
    "\n",
    "tokenizer = MidiTokenizer(VOCAB_FILE)\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "print(\"MIDI vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e9acefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11156 sequences from data/midi_text_exports\\train\n",
      "Loaded 1193 sequences from data/midi_text_exports\\val\n"
     ]
    }
   ],
   "source": [
    "from text_processing import MidiTextDataset\n",
    "from model_helpers import collate_fn\n",
    "\n",
    "\n",
    "train_dataset = MidiTextDataset(os.path.join(DATA_DIR, \"train\"), tokenizer, block_size=BLOCK_SIZE)\n",
    "val_dataset   = MidiTextDataset(os.path.join(DATA_DIR, \"val\"),   tokenizer, block_size=BLOCK_SIZE)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "416a514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained GPT-2...\n",
      "GPT-2 hidden size: 768\n",
      "Model ready. New vocab size: 640\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained GPT-2...\")\n",
    "base_model_name = \"gpt2\"  # you can try \"gpt2-medium\" if you have VRAM\n",
    "\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(base_model_name)\n",
    "base_config = pretrained_model.config\n",
    "\n",
    "hidden_size = base_config.n_embd\n",
    "print(\"GPT-2 hidden size:\", hidden_size)\n",
    "\n",
    "# New config: same architecture, new vocab size + pad/bos/eos\n",
    "new_config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=base_config.n_positions,\n",
    "    n_ctx=base_config.n_ctx,\n",
    "    n_embd=base_config.n_embd,\n",
    "    n_layer=base_config.n_layer,\n",
    "    n_head=base_config.n_head,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(new_config)\n",
    "\n",
    "# Copy transformer blocks and positional embeddings from pretrained\n",
    "with torch.no_grad():\n",
    "    # positional embeddings\n",
    "    model.transformer.wpe.weight.copy_(pretrained_model.transformer.wpe.weight)\n",
    "\n",
    "    # transformer blocks\n",
    "    for new_block, old_block in zip(model.transformer.h, pretrained_model.transformer.h):\n",
    "        new_block.load_state_dict(old_block.state_dict())\n",
    "\n",
    "    # final layer norm\n",
    "    model.transformer.ln_f.load_state_dict(pretrained_model.transformer.ln_f.state_dict())\n",
    "\n",
    "    # We intentionally leave token embeddings (wte) and lm_head randomly initialized\n",
    "    # to match new vocab.\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model ready. New vocab size:\", model.config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d67a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 465/465 [05:08<00:00,  1.51it/s, batch_loss=0.9263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | train loss: 0.9619 | val loss: 1.0214\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 465/465 [05:10<00:00,  1.50it/s, batch_loss=0.9868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | train loss: 0.8682 | val loss: 0.9644\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 465/465 [05:10<00:00,  1.50it/s, batch_loss=0.7682]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | train loss: 0.8006 | val loss: 0.9415\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 465/465 [05:07<00:00,  1.51it/s, batch_loss=0.9243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | train loss: 0.7530 | val loss: 0.9312\n",
      "  -> saving best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 465/465 [05:07<00:00,  1.51it/s, batch_loss=0.7319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | train loss: 0.7184 | val loss: 0.9246\n",
      "  -> saving best model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MidiTokenizer' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Tokenizer saving\u001b[39;00m\n\u001b[32m     69\u001b[39m os.makedirs(TOKENIZER_SAVE_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m(TOKENIZER_SAVE_DIR)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MidiTokenizer' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = NUM_EPOCHS * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=True)\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        pbar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | train loss: {avg_train_loss:.4f} | val loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        print(\"  -> saving best model\")\n",
    "        os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "        model.save_pretrained(MODEL_SAVE_DIR)\n",
    "\n",
    "# # Tokenizer saving (no longer necessary)\n",
    "# os.makedirs(TOKENIZER_SAVE_DIR, exist_ok=True)\n",
    "# tokenizer.save_pretrained(TOKENIZER_SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945e2ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated token sequence:\n",
      "<SOS> COMPOSER_mozart KEY_C TIME_SIGNATURE_4/4 TEMPO_BPM_112 MEASURE BEAT POS_0 NOTE_36 DUR_22 VEL_6 ...\n",
      "Saved generated MIDI to: generated\\gpt2_generated_sample.mid\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import torch, os\n",
    "\n",
    "def generate_midi_tokens(\n",
    "    prompt_text: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "):\n",
    "    tok = MidiTokenizer(VOCAB_FILE)\n",
    "    mdl = GPT2LMHeadModel.from_pretrained(MODEL_SAVE_DIR).to(DEVICE)\n",
    "    mdl.eval()\n",
    "\n",
    "    prompt_ids = tok.encode(prompt_text, add_special_tokens=False)\n",
    "\n",
    "    # Build input: [BOS] + prompt tokens  (no EOS)\n",
    "    input_ids = torch.tensor([[tok.bos_token_id] + prompt_ids], dtype=torch.long).to(DEVICE)\n",
    "    attention_mask = (input_ids != tok.pad_token_id).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = mdl.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "\n",
    "    generated_ids = output_ids[0].tolist()\n",
    "    generated_text = tok.decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example prompt\n",
    "example_prompt = \"\"\n",
    "generated_tokens = generate_midi_tokens(example_prompt, max_new_tokens=1024)\n",
    "print(\"Generated token sequence:\")\n",
    "print(generated_tokens[:100], \"...\" if len(generated_tokens) > 500 else \"\")\n",
    "\n",
    "# Convert generated text to MIDI and save\n",
    "generated_mid = text_to_midi(generated_tokens)\n",
    "os.makedirs(\"generated\", exist_ok=True)\n",
    "midi_path = os.path.join(\"generated\", \"gpt2_generated_sample.mid\")\n",
    "generated_mid.save(midi_path)\n",
    "print(\"Saved generated MIDI to:\", midi_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
