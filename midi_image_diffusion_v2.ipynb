{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2c451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'd:\\\\classical-music-generation-model\\\\models.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import data_preprocessing, midi_conversion, model_helpers, models\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(midi_conversion)\n",
    "importlib.reload(model_helpers)\n",
    "importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c609bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading MIDIs from data\\train.\n",
      "Could not load data\\train\\beethoven-anhang_14_3.mid: Could not decode key with 3 flats and mode 255\n",
      "Could not load data\\train\\mozart-piano_sonatas-nueva_carpeta-k281_piano_sonata_n03_3mov.mid: Could not decode key with 2 flats and mode 2\n",
      "Could not load data\\train\\unknown_artist-i_o-mozart_k550.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 500 MIDI files from data\\train\n",
      "Now loading MIDIs from data\\val.\n",
      "Loaded 47 MIDI files from data\\val\n",
      "Now loading MIDIs from data\\test.\n",
      "Could not load data\\test\\unknown_artist-i_o-mozart_q1_2.mid: MThd not found. Probably not a MIDI file\n",
      "Loaded 43 MIDI files from data\\test\n",
      "590 MIDI files retrieved.\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import get_midis_by_composer, midi_objs_to_images_with_metadata\n",
    "\n",
    "DATA_DIR = \"data/midi_image_exports\"\n",
    "MODEL_SAVE_DIR = \"models/diffusion_12000\"\n",
    "\n",
    "composers = [\"mozart\", \"haydn\", \"beethoven\"]\n",
    "midis = get_midis_by_composer(composers)\n",
    "\n",
    "train_midis, val_midis, test_midis = midis\n",
    "\n",
    "# Convert all train MIDIs to images\n",
    "train_midi_objs = [(m, c) for (m, c) in train_midis]\n",
    "midi_objs_to_images_with_metadata(train_midi_objs, DATA_DIR, \"train\")\n",
    "\n",
    "# Repeat process for validation and testing images\n",
    "val_midi_objs = [(m, c) for (m, c) in val_midis]\n",
    "midi_objs_to_images_with_metadata(val_midi_objs, DATA_DIR, \"val\")\n",
    "\n",
    "test_midi_objs = [(m, c) for (m, c) in test_midis]\n",
    "midi_objs_to_images_with_metadata(test_midi_objs, DATA_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cd9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13994 images in generated/train_images_taller\n",
      "Found 1336 images in generated/val_images_taller\n",
      "Reduced to 12000 random images.\n",
      "cuda\n",
      "\n",
      "Training for 200 epochs. Validation & checkpoint every 10 epochs.\n",
      "Early Stopping Patience: 5 checks (≈50 epochs).\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Block 1/20:  70%|███████   | 7/10 [02:52<01:13, 24.39s/it, loss=0.0423]"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import PianoRollDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from models import SimpleUNet, train_diffusion_with_early_stopping\n",
    "import torch\n",
    "\n",
    "# TRAINING ON 12,000 IMAGES (rather than 1,000)\n",
    "\n",
    "train_dataset = PianoRollDataset(f\"{DATA_DIR}/train\")\n",
    "val_dataset = PianoRollDataset(f\"{DATA_DIR}/val\")\n",
    "\n",
    "N = 12000\n",
    "perm_train = torch.randperm(len(train_dataset))[:N]\n",
    "subset_train = Subset(train_dataset, perm_train.tolist())\n",
    "print(f\"Reduced to {len(subset_train)} random images.\")\n",
    "\n",
    "train_loader = DataLoader(subset_train, batch_size=24, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "T = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = SimpleUNet().to(device)\n",
    "\n",
    "losses = train_diffusion_with_early_stopping(model, train_loader, val_loader, T, num_epochs=200, lr=1e-4, gen_freq=10,\n",
    "                                             device=device, save_dir=\"models\\diffusion_12000\", weight_decay=1e-4, patience=5,\n",
    "                                             save_checkpoints=True, alpha_start=0.5, alpha_end=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7ebdcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 26004 images in data/midi_image_exports/train\n",
      "Found 2469 images in data/midi_image_exports/val\n",
      "Reduced to 12000 random images.\n"
     ]
    }
   ],
   "source": [
    "from data_preprocessing import PianoRollDatasetWithMetadata\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import util\n",
    "\n",
    "# 1. Define Mappings\n",
    "COMPOSER_MAP = {\n",
    "    \"haydn\": 0,\n",
    "    \"mozart\": 1,\n",
    "    \"beethoven\": 2,\n",
    "    \"null\": 3\n",
    "}\n",
    "KEY_MAP = {\n",
    "    \"C\": 0, \"Cm\": 1, \"Db\": 2, \"C#m\": 3, \"D\": 4, \"Dm\": 5,\n",
    "    \"Eb\": 6, \"Ebm\": 7, \"E\": 8, \"Em\": 9, \"F\": 10, \"Fm\": 11, \"Gb\": 12,\n",
    "    \"F#m\": 13, \"G\": 14, \"Gm\": 15, \"Ab\": 16, \"G#m\": 17, \"A\": 18,\n",
    "    \"Am\": 19, \"Bb\": 20, \"Bbm\": 21, \"B\": 22, \"Bm\": 23, \"Unknown\": 24, \"null\": 25 # NULL token\n",
    "}\n",
    "\n",
    "device = util.get_best_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = PianoRollDatasetWithMetadata(f\"{DATA_DIR}/train\", COMPOSER_MAP, KEY_MAP)\n",
    "val_dataset = PianoRollDatasetWithMetadata(f\"{DATA_DIR}/val\", COMPOSER_MAP, KEY_MAP)\n",
    "\n",
    "# N = 12,000 random IMAGES in training subset\n",
    "N = 12000\n",
    "if len(train_dataset) > N:\n",
    "    perm_train = torch.randperm(len(train_dataset))[:N]\n",
    "    train_dataset = Subset(train_dataset, perm_train.tolist())\n",
    "\n",
    "batch_size = 24\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Reduced to {len(train_loader)*batch_size} random images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf5fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ConditionedUNet, train_cfg_diffusion_with_early_stopping\n",
    "\n",
    "# Initialize the model\n",
    "model = ConditionedUNet(\n",
    "    channels=[16, 32, 64, 128],\n",
    "    num_composers=len(COMPOSER_MAP),\n",
    "    num_keys=len(KEY_MAP)\n",
    ").to(device)\n",
    "\n",
    "# Train\n",
    "# Note: num_composers and num_keys must match the len() of your maps\n",
    "losses = train_cfg_diffusion_with_early_stopping(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    timesteps=100, \n",
    "    num_composers=len(COMPOSER_MAP),\n",
    "    num_keys=len(KEY_MAP),\n",
    "    num_epochs=200, \n",
    "    lr=1e-4, \n",
    "    device=device,\n",
    "    save_dir=\"models/diffusion_conditional_1200\",\n",
    "    alpha_start=0.0, \n",
    "    alpha_end=0.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci372",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
